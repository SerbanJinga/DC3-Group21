{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Topic modelling using BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Libraries/data required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20212324\\PycharmProjects\\DC3-Group21\\venv\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20212324\\PycharmProjects\\DC3-Group21\\venv\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20212324\\PycharmProjects\\DC3-Group21\\venv\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\20212324\\PycharmProjects\\DC3-Group21\\venv\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18520, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>date</th>\n",
       "      <th>location_article</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The article discusses the passing of the new C...</td>\n",
       "      <td>2011-07-07</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The article discusses the military actions tak...</td>\n",
       "      <td>2011-07-03</td>\n",
       "      <td>Abyei</td>\n",
       "      <td>9.838551</td>\n",
       "      <td>28.486396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The article discusses the signing of a Framewo...</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>Southern Kordofan</td>\n",
       "      <td>11.036544</td>\n",
       "      <td>30.895824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The article discusses the upcoming independenc...</td>\n",
       "      <td>2011-07-04</td>\n",
       "      <td>South Sudan</td>\n",
       "      <td>6.876992</td>\n",
       "      <td>31.306979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The article discusses the need for South Sudan...</td>\n",
       "      <td>2011-07-02</td>\n",
       "      <td>Juba</td>\n",
       "      <td>4.859363</td>\n",
       "      <td>31.571250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary       date  \\\n",
       "0  The article discusses the passing of the new C... 2011-07-07   \n",
       "1  The article discusses the military actions tak... 2011-07-03   \n",
       "2  The article discusses the signing of a Framewo... 2011-06-30   \n",
       "3  The article discusses the upcoming independenc... 2011-07-04   \n",
       "4  The article discusses the need for South Sudan... 2011-07-02   \n",
       "\n",
       "    location_article        lat        lng  \n",
       "0               Juba   4.859363  31.571250  \n",
       "1              Abyei   9.838551  28.486396  \n",
       "2  Southern Kordofan  11.036544  30.895824  \n",
       "3        South Sudan   6.876992  31.306979  \n",
       "4               Juba   4.859363  31.571250  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and perform preprocessing\n",
    "\n",
    "df = pd.read_csv(\"data/articles_summary_cleaned.csv\", parse_dates=[\"date\"]) # Read data into 'df' dataframe\n",
    "print(df.shape) # Print dataframe shape\n",
    "\n",
    "docs = df[\"summary\"].tolist() # Create a list containing all article summaries\n",
    "\n",
    "df.head() # Show first 5 dataframe entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fitting BERTopic\n",
    "\n",
    "This might take a while on a CPU. In the background a pre-trained Large Language Model, called the sentence embedder, is used to convert the articles to a semantic vector space. We then perform clustering in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('southsudan_model'):\n",
    "    bertopic = BERTopic.load('southsudan_model')\n",
    "else:\n",
    "    bertopic = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True) # Initialize the BERTopic model\n",
    "\n",
    "    bertopic.fit_transform(docs) # Fit the model to the list of article summaries\n",
    "    bertopic.save(\"southsudan_model\") # Save the trained model as \"southsudan_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Due to the modularity of the model, there is a lot of randomness that hinders reproducibiity of the model.\n",
    "#To fight this, you can for example set random state in the dimensionality reduction step via the following lines \n",
    "#or explore a different approach\n",
    "\n",
    "#from bertopic import BERTopic\n",
    "#from umap import UMAP\n",
    "\n",
    "#umap_model = UMAP(n_neighbors=15, n_components=5, \n",
    "#                  min_dist=0.0, metric='cosine', random_state=42)\n",
    "#topic_model = BERTopic(umap_model=umap_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# changing bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model2'):\n",
    "    bertopic_standard = BERTopic.load('southsudan_model2')\n",
    "else:\n",
    "    bertopic_standard = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model) # Initialize the BERTopic model\n",
    "    bertopic_standard.fit_transform(docs) # Fit the model to the list of article summaries\n",
    "    bertopic_standard.save(\"southsudan_model2\") # Save the trained model as \"southsudan_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>6559</td>\n",
       "      <td>-1_the_and_of_to</td>\n",
       "      <td>[the, and, of, to, in, south, sudan, article, ...</td>\n",
       "      <td>[The article discusses the United States' deep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>0_food_wfp_million_hunger</td>\n",
       "      <td>[food, wfp, million, hunger, people, famine, a...</td>\n",
       "      <td>[The article discusses how USAID has provided ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>355</td>\n",
       "      <td>1_kiir_his_author_president</td>\n",
       "      <td>[kiir, his, author, president, salva, kiirs, c...</td>\n",
       "      <td>[The article discusses the political crisis in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>331</td>\n",
       "      <td>2_abyei_referendum_ngok_area</td>\n",
       "      <td>[abyei, referendum, ngok, area, misseriya, din...</td>\n",
       "      <td>[The article discusses the African Union Peace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>227</td>\n",
       "      <td>3_million_aid_humanitarian_funding</td>\n",
       "      <td>[million, aid, humanitarian, funding, people, ...</td>\n",
       "      <td>[The article discusses the US President author...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>240</td>\n",
       "      <td>10</td>\n",
       "      <td>240_kalaazar_disease_azar_kala</td>\n",
       "      <td>[kalaazar, disease, azar, kala, flies, cases, ...</td>\n",
       "      <td>[The article discusses the outbreak of Kala-az...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>241</td>\n",
       "      <td>10</td>\n",
       "      <td>241_refugees_batil_hygiene_respiratory</td>\n",
       "      <td>[refugees, batil, hygiene, respiratory, nile, ...</td>\n",
       "      <td>[The article discusses the alarming health sit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>242</td>\n",
       "      <td>10</td>\n",
       "      <td>242_comic_defections_conflict_salva</td>\n",
       "      <td>[comic, defections, conflict, salva, political...</td>\n",
       "      <td>[The article discusses the ongoing conflict be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>243</td>\n",
       "      <td>10</td>\n",
       "      <td>243_children_unicef_families_app</td>\n",
       "      <td>[children, unicef, families, app, million, str...</td>\n",
       "      <td>[The article discusses how violence and insecu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>244</td>\n",
       "      <td>10</td>\n",
       "      <td>244_issues_outstanding_border_omer</td>\n",
       "      <td>[issues, outstanding, border, omer, two, union...</td>\n",
       "      <td>[The article discusses the upcoming meeting be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                    Name  \\\n",
       "0       -1   6559                        -1_the_and_of_to   \n",
       "1        0    377               0_food_wfp_million_hunger   \n",
       "2        1    355             1_kiir_his_author_president   \n",
       "3        2    331            2_abyei_referendum_ngok_area   \n",
       "4        3    227      3_million_aid_humanitarian_funding   \n",
       "..     ...    ...                                     ...   \n",
       "241    240     10          240_kalaazar_disease_azar_kala   \n",
       "242    241     10  241_refugees_batil_hygiene_respiratory   \n",
       "243    242     10     242_comic_defections_conflict_salva   \n",
       "244    243     10        243_children_unicef_families_app   \n",
       "245    244     10      244_issues_outstanding_border_omer   \n",
       "\n",
       "                                        Representation  \\\n",
       "0    [the, and, of, to, in, south, sudan, article, ...   \n",
       "1    [food, wfp, million, hunger, people, famine, a...   \n",
       "2    [kiir, his, author, president, salva, kiirs, c...   \n",
       "3    [abyei, referendum, ngok, area, misseriya, din...   \n",
       "4    [million, aid, humanitarian, funding, people, ...   \n",
       "..                                                 ...   \n",
       "241  [kalaazar, disease, azar, kala, flies, cases, ...   \n",
       "242  [refugees, batil, hygiene, respiratory, nile, ...   \n",
       "243  [comic, defections, conflict, salva, political...   \n",
       "244  [children, unicef, families, app, million, str...   \n",
       "245  [issues, outstanding, border, omer, two, union...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [The article discusses the United States' deep...  \n",
       "1    [The article discusses how USAID has provided ...  \n",
       "2    [The article discusses the political crisis in...  \n",
       "3    [The article discusses the African Union Peace...  \n",
       "4    [The article discusses the US President author...  \n",
       "..                                                 ...  \n",
       "241  [The article discusses the outbreak of Kala-az...  \n",
       "242  [The article discusses the alarming health sit...  \n",
       "243  [The article discusses the ongoing conflict be...  \n",
       "244  [The article discusses how violence and insecu...  \n",
       "245  [The article discusses the upcoming meeting be...  \n",
       "\n",
       "[246 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertopic_standard.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PCA dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA(random_state=42, n_components=300)\n",
    "\n",
    "#if os.path.exists('southsudan_model3'):\n",
    "#    bertopic_PCA = BERTopic.load('southsudan_model3')\n",
    "#else:\n",
    "bertopic_PCA = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=pca_model)\n",
    "bertopic_PCA.fit_transform(docs)\n",
    "bertopic_PCA.save(\"southsudan_model3\")\n",
    "\n",
    "bertopic_PCA.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KMeans clustering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=250, random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model4'):\n",
    "    bertopic_KM = BERTopic.load('southsudan_model4')\n",
    "else:\n",
    "    bertopic_KM = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, hdbscan_model=kmeans_model)\n",
    "    bertopic_KM.fit_transform(docs)\n",
    "    bertopic_KM.save(\"southsudan_model4\")\n",
    "\n",
    "bertopic_KM.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA with KMeans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_model = PCA(random_state=42, n_components=50)\n",
    "kmeans_model = KMeans(n_clusters=50, random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model5'):\n",
    "    bertopic_PCA_KM = BERTopic.load('southsudan_model5')\n",
    "else:\n",
    "    bertopic_PCA_KM = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=pca_model, hdbscan_model=kmeans_model)\n",
    "    bertopic_PCA_KM.fit_transform(docs)\n",
    "    bertopic_PCA_KM.save(\"southsudan_model5\")\n",
    "\n",
    "bertopic_PCA_KM.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## change the range of n-grams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists('southsudan_model6'):\n",
    "    bertopic_NG = BERTopic.load('southsudan_model6')\n",
    "else:\n",
    "    bertopic_NG = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, n_gram_range=(1, 3))\n",
    "    bertopic_NG.fit_transform(docs)\n",
    "    bertopic_NG.save(\"southsudan_model6\")\n",
    "\n",
    "bertopic_NG.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA and n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_model = PCA(random_state=42, n_components=150)\n",
    "\n",
    "if os.path.exists('southsudan_model7'):\n",
    "    bertopic_PCA_NG = BERTopic.load('southsudan_model7')\n",
    "else:\n",
    "    bertopic_PCA_NG = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=pca_model, n_gram_range=(1, 3))\n",
    "    bertopic_PCA_NG.fit_transform(docs)\n",
    "    bertopic_PCA_NG.save(\"southsudan_model7\")\n",
    "\n",
    "bertopic_PCA_NG.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KMeans and n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kmeans_model = KMeans(n_clusters=150, random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model8'):\n",
    "    bertopic_KM_NG = BERTopic.load('southsudan_model8')\n",
    "else:\n",
    "    bertopic_KM_NG = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, hdbscan_model=kmeans_model, n_gram_range=(1, 3))\n",
    "    bertopic_KM_NG.fit_transform(docs)\n",
    "    bertopic_KM_NG.save(\"southsudan_model8\")\n",
    "\n",
    "bertopic_KM_NG.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA and KMeans and n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_model = PCA(random_state=42, n_components=50)\n",
    "kmeans_model = KMeans(n_clusters=50, random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model9'):\n",
    "    bertopic_PCA_KM_NG = BERTopic.load('southsudan_model9')\n",
    "else:\n",
    "    bertopic_PCA_KM_NG = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=pca_model, hdbscan_model=kmeans_model, n_gram_range=(1, 3))\n",
    "    bertopic_PCA_KM_NG.fit_transform(docs)\n",
    "    bertopic_PCA_KM_NG.save(\"southsudan_model9\")\n",
    "\n",
    "bertopic_PCA_KM_NG.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# remove stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "if os.path.exists('southsudan_model10'):\n",
    "    bertopic_VM = BERTopic.load('southsudan_model10')\n",
    "else:\n",
    "    bertopic_VM = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, vectorizer_model=vectorizer_model)\n",
    "    bertopic_VM.fit_transform(docs)\n",
    "    bertopic_VM.save(\"southsudan_model10\")\n",
    "\n",
    "bertopic_VM.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove stopwords and KM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "kmeans_model = KMeans(n_clusters=150, random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model11'):\n",
    "    bertopic_VM_KM = BERTopic.load('southsudan_model11')\n",
    "else:\n",
    "    bertopic_VM_KM = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, vectorizer_model=vectorizer_model, hdbscan_model=kmeans_model)\n",
    "    bertopic_VM_KM.fit_transform(docs)\n",
    "    bertopic_VM_KM.save(\"southsudan_model11\")\n",
    "\n",
    "bertopic_VM_KM.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove stopwords PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pca_model = PCA(random_state=42, n_components=150)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "if os.path.exists('southsudan_model18'):\n",
    "    bertopic_VM_PCA = BERTopic.load('southsudan_model18')\n",
    "else:\n",
    "    bertopic_VM_PCA = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=pca_model, vectorizer_model=vectorizer_model)\n",
    "    bertopic_VM_PCA.fit_transform(docs)\n",
    "    bertopic_VM_PCA.save(\"southsudan_model18\")\n",
    "\n",
    "bertopic_VM_PCA.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove stopwords and PCA and KM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pca_model = PCA(random_state=42, n_components=50)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "kmeans_model = KMeans(n_clusters=50, random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model12'):\n",
    "    bertopic_VM_KM_PCA = BERTopic.load('southsudan_model12')\n",
    "else:\n",
    "    bertopic_VM_KM_PCA = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=pca_model, vectorizer_model=vectorizer_model, hdbscan_model=kmeans_model)\n",
    "    bertopic_VM_KM_PCA.fit_transform(docs)\n",
    "    bertopic_VM_KM_PCA.save(\"southsudan_model12\")\n",
    "\n",
    "bertopic_VM_KM_PCA.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove stopwords and KM and n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "kmeans_model = KMeans(n_clusters=150, random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model16'):\n",
    "    bertopic_VM_KM_NG = BERTopic.load('southsudan_model16')\n",
    "else:\n",
    "    bertopic_VM_KM_NG = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, vectorizer_model=vectorizer_model, hdbscan_model=kmeans_model, n_gram_range=(1, 3))\n",
    "    bertopic_VM_KM_NG.fit_transform(docs)\n",
    "    bertopic_VM_KM_NG.save(\"southsudan_model16\")\n",
    "\n",
    "bertopic_VM_KM_NG.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove stopwords and PCA and n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "pca_model = PCA(random_state=42, n_components=150)\n",
    "\n",
    "#if os.path.exists('southsudan_model19'):\n",
    "#    bertopic_VM_PCA_NG = BERTopic.load('southsudan_model19')\n",
    "#else:\n",
    "bertopic_VM_PCA_NG = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, vectorizer_model=vectorizer_model, umap_model=pca_model, n_gram_range=(1, 3))\n",
    "bertopic_VM_PCA_NG.fit_transform(docs)\n",
    "bertopic_VM_PCA_NG.save(\"southsudan_model19\")\n",
    "\n",
    "bertopic_VM_PCA_NG.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove stopwords and PCA and KM and n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pca_model = PCA(random_state=42, n_components=50)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "kmeans_model = KMeans(n_clusters=50, random_state=42)\n",
    "\n",
    "if os.path.exists('southsudan_model13'):\n",
    "    bertopic_VM_KM_PCA_NG = BERTopic.load('southsudan_model13')\n",
    "else:\n",
    "    bertopic_VM_KM_PCA_NG = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=pca_model, vectorizer_model=vectorizer_model, hdbscan_model=kmeans_model, n_gram_range=(1, 3))\n",
    "    bertopic_VM_KM_PCA_NG.fit_transform(docs)\n",
    "    bertopic_VM_KM_PCA_NG.save(\"southsudan_model13\")\n",
    "\n",
    "bertopic_VM_KM_PCA_NG.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## standard model, remove stopwords and n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "if os.path.exists('southsudan_model14'):\n",
    "    bertopic_VM_NG = BERTopic.load('southsudan_model14')\n",
    "else:\n",
    "    bertopic_VM_NG = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, vectorizer_model=vectorizer_model, n_gram_range=(1, 3))\n",
    "    bertopic_VM_NG.fit_transform(docs)\n",
    "    bertopic_VM_NG.save(\"southsudan_model14\")\n",
    "\n",
    "bertopic_VM_NG.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## standard model, remove stopwords and n-gram, diversify"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.7)\n",
    "\n",
    "if os.path.exists('southsudan_model15'):\n",
    "    bertopic_VM_NG_D = BERTopic.load('southsudan_model15')\n",
    "else:\n",
    "    bertopic_VM_NG_D = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, vectorizer_model=vectorizer_model, n_gram_range=(1, 3), representation_model=representation_model)\n",
    "    bertopic_VM_NG_D.fit_transform(docs)\n",
    "    bertopic_VM_NG_D.save(\"southsudan_model15\")\n",
    "\n",
    "bertopic_VM_NG_D.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove stopwords and KM and n-gram, diversify"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "kmeans_model = KMeans(n_clusters=50, random_state=42)\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.7)\n",
    "\n",
    "if os.path.exists('southsudan_model17'):\n",
    "    bertopic_VM_KM_NG_D = BERTopic.load('southsudan_model17')\n",
    "else:\n",
    "    bertopic_VM_KM_NG_D = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, umap_model=umap_model, vectorizer_model=vectorizer_model, hdbscan_model=kmeans_model, n_gram_range=(1, 3), representation_model=representation_model)\n",
    "    bertopic_VM_KM_NG_D.fit_transform(docs)\n",
    "    bertopic_VM_KM_NG_D.save(\"southsudan_model17\")\n",
    "\n",
    "bertopic_VM_KM_NG_D.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## remove stopwords and PCA and n-gram, diversify"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "pca_model = PCA(random_state=42, n_components=150)\n",
    "representation_model = MaximalMarginalRelevance(diversity=1)\n",
    "\n",
    "if os.path.exists('southsudan_model20'):\n",
    "    bertopic_VM_PCA_NG_D = BERTopic.load('southsudan_model20')\n",
    "else:\n",
    "    bertopic_VM_PCA_NG_D = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, vectorizer_model=vectorizer_model, umap_model=pca_model, n_gram_range=(1, 3), representation_model=representation_model)\n",
    "    bertopic_VM_PCA_NG_D.fit_transform(docs)\n",
    "    bertopic_VM_PCA_NG_D.save(\"southsudan_model20\")\n",
    "\n",
    "bertopic_VM_PCA_NG_D.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## wordcloud"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_wordcloud(model, topic):\n",
    "    text = {word: value for word, value in model.get_topic(topic)}\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    wc.generate_from_frequencies(text)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Show wordcloud\n",
    "create_wordcloud(bertopic_VM_NG_D, topic=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interactive visualization of the vector space\n",
    "\n",
    "As you can see, documents with related topics are close in the space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bertopic.visualize_documents(docs) # Create a plot of the topics, this may take a while"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating smaller topics\n",
    "\n",
    "Within our list of topics, we find topics that are semantically closest to 4 keywords:\n",
    "\n",
    "\"Hunger\", \"Refugees\", \"Conflict\", and \"Humanitarian\".\n",
    "\n",
    "**Feel free to change this approach!**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We create a function to calculate a list of the top n topics related to (a) given keyword(s)\n",
    "\n",
    "def get_relevant_topics(bertopic_model, keywords, top_n):\n",
    "    '''\n",
    "    Retrieve a list of the top n number of relevant topics to the provided (list of) keyword(s)\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        bertopic_model: a (fitted) BERTopic model object\n",
    "        \n",
    "        keywords:   a string containing one or multiple keywords to match against,\n",
    "                    \n",
    "                    This can also be a list in the form of ['keyword(s)', keyword(s), ...]\n",
    "                    \n",
    "                    In this case a maximum of top_n topics will be found per list element \n",
    "                    and subsetted to the top_n most relevant topics.\n",
    "                    \n",
    "                    !!!\n",
    "                    Take care that this method only considers the relevancy per inputted keyword(s) \n",
    "                    and not the relevancy to the combined list of keywords.\n",
    "                    \n",
    "                    In other words, topics that appear in the output might be significantly related to a \n",
    "                    particular element in the list of keywords but not so to any other element, \n",
    "                    \n",
    "                    while topics that do not appear in the output might be significantly related to the \n",
    "                    combined list of keywords but not much to any of the keyword(s) in particular.\n",
    "                    !!!\n",
    "                    \n",
    "        top_n: an integer indicating the number of desired relevant topics to be retrieved\n",
    "        \n",
    "        \n",
    "        Return: a list of the top_n (or less) topics most relevant to the (list of) provided keyword(s)\n",
    "    '''\n",
    "    \n",
    "    if type(keywords) is str: keywords = [keywords] # If a single string is provided convert it to list type\n",
    "    \n",
    "    relevant_topics = list() # Initilize an empty list of relevant topics\n",
    "    \n",
    "    for keyword in keywords: # Iterate through list of keywords\n",
    "        \n",
    "        # Find the top n number of topics related to the current keyword(s)\n",
    "        topics = bertopic_model.find_topics(keyword, top_n = top_n)\n",
    "        \n",
    "        # Add the topics to the list of relevant topics in the form of (topic_id, relevancy)\n",
    "        relevant_topics.extend(\n",
    "            zip(topics[0], topics[1]) # topics[0] = topic_id, topics[1] = relevancy\n",
    "        )\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1]) # Sort the list of topics on ASCENDING ORDER of relevancy\n",
    "    \n",
    "    # Get a list of the set of unique topics (with greates relevancy in case of duplicate topics)\n",
    "    relevant_topics = list(dict(relevant_topics).items())\n",
    "    \n",
    "    \n",
    "    relevant_topics.sort(key=lambda x: x[1], reverse=True) # Now sort the list of topics on DESCENDING ORDER of relevancy\n",
    "    \n",
    "    return relevant_topics[:10] # Return a list of the top_n unique relevant topics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## create dfs for the different models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def topics_related_to_keywords(df, model, keywords_list, end_keyword):\n",
    "    relevant_topics = get_relevant_topics(bertopic_model = model, keywords=keywords_list, top_n=10)\n",
    "\n",
    "    topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "    \n",
    "    df[end_keyword] = [t in topic_ids for t in model.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "    \n",
    "    # View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "    model.get_topic_info().set_index('Topic').loc[topic_ids]\n",
    "    return df\n",
    "\n",
    "models_dict = {\n",
    "    'bertopic_standard': bertopic_standard,\n",
    "    'bertopic_PCA': bertopic_PCA,\n",
    "    'bertopic_KM': bertopic_KM,\n",
    "    'bertopic_PCA_KM': bertopic_PCA_KM,\n",
    "    'bertopic_NG': bertopic_NG,\n",
    "    'bertopic_PCA_NG': bertopic_PCA_NG,\n",
    "    'bertopic_KM_NG': bertopic_KM_NG,\n",
    "    'bertopic_PCA_KM_NG': bertopic_PCA_KM_NG,\n",
    "    'bertopic_VM': bertopic_VM,\n",
    "    'bertopic_VM_KM': bertopic_VM_KM,\n",
    "    'bertopic_VM_KM_PCA': bertopic_VM_KM_PCA,\n",
    "    'bertopic_VM_KM_NG': bertopic_VM_KM_NG,\n",
    "    'bertopic_VM_KM_PCA_NG': bertopic_VM_KM_PCA_NG,\n",
    "    'bertopic_VM_NG': bertopic_VM_NG, \n",
    "    'bertopic_VM_NG_D': bertopic_VM_NG_D, \n",
    "    'bertopic_VM_KM_NG_D': bertopic_VM_KM_NG_D,\n",
    "    'bertopic_VM_PCA': bertopic_VM_PCA,\n",
    "    'bertopic_VM_PCA_NG': bertopic_VM_PCA_NG,\n",
    "    'bertopic_VM_PCA_NG_D': bertopic_VM_PCA_NG_D\n",
    "}\n",
    "\n",
    "keyw = {\"hunger\": ['hunger', 'food insecurity'], \n",
    "        \"refugees\": ['refugees', 'displaced'], \n",
    "        \"humanitarian\": [\"humanitarian\"], \n",
    "        \"conflict\": ['conflict', 'fighting', 'murder']}\n",
    "keys_list = list(keyw.keys())\n",
    "\n",
    "for name, mod in models_dict.items():\n",
    "    df = pd.read_csv(\"data/articles_summary_cleaned.csv\", parse_dates=[\"date\"])\n",
    "    df_name = 'df_' + name\n",
    "    for keyword, keywlist in keyw.items():\n",
    "        df = topics_related_to_keywords(df, mod, keywlist, keyword)\n",
    "    original_df = pd.read_csv(\"data/articles_summary_cleaned.csv\", parse_dates=[\"date\"])\n",
    "    df_merge = original_df.merge(df[['summary'] + keys_list], how=\"left\", left_on=\"summary\", right_on=\"summary\")\n",
    "    df_merge.to_csv(\"data/\"+str(name)+\".csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'hunger' and 'food insecurity'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic_standard, keywords=['hunger', 'food insecurity'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"hunger\"] = [t in topic_ids for t in bertopic_standard.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic_standard.get_topic_info().set_index('Topic').loc[topic_ids]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'refugees' and 'displaced'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic_standard, keywords=['refugees', 'displaced'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"refugees\"] = [t in topic_ids for t in bertopic_standard.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic_standard.get_topic_info().set_index('Topic').loc[topic_ids]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keyword 'humanitarian'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic_standard, keywords=['humanitarian'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"humanitarian\"] = [t in topic_ids for t in bertopic_standard.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic_standard.get_topic_info().set_index('Topic').loc[topic_ids]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the top 10 topics related to the keywords 'conflict', 'fighting', and 'murder'\n",
    "relevant_topics = get_relevant_topics(bertopic_model = bertopic_standard, keywords=['conflict', 'fighting', 'murder'], top_n=10)\n",
    "\n",
    "topic_ids = [el[0] for el in relevant_topics] # Create seperate list of topic IDs\n",
    "\n",
    "for topic_id, relevancy in relevant_topics: # Print neat list of (topic_id, relevancy) tuples\n",
    "    print(topic_id, relevancy)\n",
    "    \n",
    "df[\"conflict\"] = [t in topic_ids for t in bertopic_standard.topics_] # Add boolean column to df if topic in list of relevant topics\n",
    "\n",
    "# View the Count, Name, Representation, and Representative Docs for the relevant topics\n",
    "bertopic_standard.get_topic_info().set_index('Topic').loc[topic_ids]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}